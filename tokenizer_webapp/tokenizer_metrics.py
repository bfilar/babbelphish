import argparse
import base64

import blobfile as bf
import jsonlines
import tiktoken


def load_dataset(file_path):
    """Load the dataset from a JSONL file."""
    dataset = []
    with jsonlines.open(file_path) as reader:
        for record in reader:
            prompt = record["prompt"]
            completion = record["completion"]
            dataset.append(prompt)
            dataset.append(completion)
    return dataset


def load_tokenizer_vocab(bpe_blobpath):
    """Load the vocabulary from the tokenizer."""
    with bf.BlobFile(bpe_blobpath, "rb") as f:
        bpe_contents = f.read()
    tokenizer_vocab = load_vocab_from_bytes(bpe_contents)
    return tokenizer_vocab


def load_vocab_from_bytes(bpe_vocab_bytes):
    """
    Load the BPE vocab from bytes and return a set of tokens.

    Args:
    bpe_vocab_bytes (bytes): The bytes representing the BPE vocab.

    Returns:
    set: A set of all tokens in the vocab.
    """
    # Decode the bytes into a string
    bpe_vocab_str = bpe_vocab_bytes.decode("utf-8")

    # Split the string into lines and discard the first line (which contains the version)
    lines = bpe_vocab_str.split("\n")[1:]

    # Each line contains a token and a frequency, separated by a space.
    # We just need the tokens, so we split each line into parts and keep the first part.
    tokens = {base64.b64decode(line.split(" ")[0]) for line in lines if line}

    return tokens


def preprocess_dataset(dataset):
    """
    Preprocess the dataset by tokenizing each sentence into words.

    Args:
    dataset (list): The list of sentences.

    Returns:
    list: A list of all words in the dataset.
    """
    words = []
    for sentence in dataset:
        words.extend(sentence.split())
    return words


def calculate_oov_rate(tokenizer_vocab, dataset):
    """
    Calculate the rate of words not found in the tokenizer's vocabulary.

    Args:
    tokenizer_vocab (list): The vocabulary of the tokenizer.
    dataset (list): The dataset to be tokenized.

    Returns:
    float: The rate of out-of-vocabulary words.
    """
    unknown_words = []
    for word in dataset:
        if (
            bytes(" " + word, "utf-8") not in tokenizer_vocab
        ):  # convert words to bytes before checking
            unknown_words.append(word)
    oov_rate = len(unknown_words) / len(dataset)
    return oov_rate


def calculate_tokenization_granularity(tokenizer, dataset):
    """
    Analyze the tokenization process and calculate how often the tokenizer
    splits an identifier into multiple tokens.

    Args:
    tokenizer (object): The tokenizer to use.
    dataset (list): The dataset to be tokenized.

    Returns:
    float: The rate of split identifiers.
    """
    split_tokens_count = 0
    for identifier in dataset:
        token_ids = tokenizer.encode(identifier)
        if len(token_ids) > 1:
            split_tokens_count += 1
    granularity = split_tokens_count / len(dataset)
    return granularity


def calculate_information_loss(tokenizer, dataset):
    """
    Calculate if the tokenization process results in loss of information.

    Args:
    tokenizer (object): The tokenizer to use.
    dataset (list): The dataset to be tokenized.

    Returns:
    float: The rate of information loss.
    """
    loss_count = 0
    for sentence in dataset:
        token_ids = tokenizer.encode(sentence)
        reconstructed_sentence = tokenizer.decode(token_ids)
        if sentence != reconstructed_sentence:
            loss_count += 1
    info_loss = loss_count / len(dataset)
    return info_loss


def calculate_token_type_ratio(tokenizer, dataset):
    """
    Measure the balance between the number of unique tokens and the total
    number of tokens generated by the tokenizer.

    Args:
    tokenizer (object): The tokenizer to use.
    dataset (list): The dataset to be tokenized.

    Returns:
    float: The token type ratio.
    """
    all_tokens = []
    for sentence in dataset:
        token_ids = tokenizer.encode(sentence)
        all_tokens.extend(token_ids)
    unique_tokens = set(all_tokens)
    token_type_ratio = len(unique_tokens) / len(all_tokens)
    return token_type_ratio


def calculate_reversibility(tokenizer, dataset):
    """
    Calculate if the original input can be perfectly reconstructed from
    the tokenized output.

    Args:
    tokenizer (object): The tokenizer to use.
    dataset (list): The dataset to be tokenized.

    Returns:
    float: The rate of non-reversible sentences.
    """
    not_reversible_count = 0
    for sentence in dataset:
        token_ids = tokenizer.encode(sentence)
        reconstructed_sentence = tokenizer.decode(token_ids)
        if sentence != reconstructed_sentence:
            not_reversible_count += 1
    reversibility = not_reversible_count / len(dataset)
    return reversibility


def main(args):
    tokenizer = tiktoken.get_encoding(args.tokenizer_type)
    tokenizer_vocab = load_tokenizer_vocab(args.bpe_blobpath)

    dataset = load_dataset(args.dataset_path)
    dataset = preprocess_dataset(dataset)

    oov_rate = calculate_oov_rate(tokenizer_vocab, dataset)
    granularity = calculate_tokenization_granularity(tokenizer, dataset)
    info_loss = calculate_information_loss(tokenizer, dataset)
    token_type_ratio = calculate_token_type_ratio(tokenizer, dataset)
    reversibility = calculate_reversibility(tokenizer, dataset)

    print(f"OOV rate: {oov_rate}")
    print(f"Tokenization granularity: {granularity}")
    print(f"Information loss: {info_loss}")
    print(f"Token type ratio: {token_type_ratio}")
    print(f"Reversibility: {reversibility}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Calculate tokenizer metrics.")
    parser.add_argument(
        "--tokenizer_type",
        type=str,
        default="r50k_base",
        help="The type of the tokenizer to use.",
    )
    parser.add_argument(
        "--bpe_blobpath",
        type=str,
        default="az://openaipublic/encodings/r50k_base.tiktoken",
        help="The path to the BPE encoding file.",
    )
    parser.add_argument(
        "--dataset_path",
        type=str,
        default="data/mql_prompts_prepared.jsonl",
        help="The path to the dataset file.",
    )
    args = parser.parse_args()

    main(args)
